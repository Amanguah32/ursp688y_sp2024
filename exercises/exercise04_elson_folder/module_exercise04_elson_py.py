# -*- coding: utf-8 -*-
"""module_exercise04_elson.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-C0cV_n1ZUfyZnB612aYk0RAzT_ggwU

Urban Data Science & Smart Cities <br>
URSP688Y <br>
Instructor: Chester Harvey <br>
Urban Studies & Planning <br>
National Center for Smart Growth <br>
University of Maryland

[<img src="https://colab.research.google.com/assets/colab-badge.svg">](https://colab.research.google.com/github/ncsg/ursp688y_sp2024/blob/main/exercises/exercise04/exercise04.ipynb)

# Exercise 4 (in Two Parts)

# The Data Viz Part

Next week is Data Visualization week. This is one of my favorite topics, in part because we get to look at lots of pictures, and in part because it provides an excuse for some very lighthearted competition.

In prep for next week, part of your exercise is to find an example of either an _excellent_ or _terrible_ data visualization. We will vote on the best (and worst) in each category, and the winner gets a small (tasty) prize.

Please find an example of a data visualization that is either _very effective_ or _terribly ineffective_ in communicating an interesting finding from data. Here are a few ground rules:
- One figure only: We should be able to see the whole thing at once on a projector screen.
- Static images only: If you find something dynamic or interactive that you _must_ submit, please take a screenshot.
- Do the reading first: Tufte will give you some ideas for what makes visualizations good or bad
- No examples from Tufte. Gotta work a little bit.

Please either paste a link to your image in the text cell below (can you figure out how to get markdown to display the image?) or add an image file to your PR.
- Please label it clearly as "good" or "bad" so we know which race you're in.
- Please write a couple bullets about why it's good or bad. This is your pitch (we can haggle about it in class, too.)

## Good/Bad (please choose one and delete the other)
- Why
- Some more why
- Any more?

***** Put image link or insert image here *****

# The Programming Part

## Problem

In [Exercise 3](https://github.com/ncsg/ursp688y_sp2024/blob/main/exercises/exercise03/exercise03.ipynb), you examined how many affordable housing units available to households up to 60% AMI were planned within each ward in Washington, D.C.

The bonus problem was to calculate which wards were producing a _disproportionately_ large and small number of housing units given their populations.

This week, please reproduce this analysis, <ins>including</ins> the bonus part, using some of your new data loading, joining, and module-building skills.

Please write a program that:

- Loads the affordable housing project data from `affordable_housing.csv`
- Loads the ward populations from `wards_from_2022.csv`
- Joins the population data to the affordable housing data
- Calculates which wards are producing disproportionately large and small number of housing units given their populations
- Completes all of this data loading and processing within a function (or a series of functions called by a single main function)
- Stores that function (and any related functions) in a module
- Calls the main function in the exercise notebook to return table or other summary or results

## Data

CSVs for both required data tables are included on GitHub at `exercises/exercise04`.

Please consult the city's database of [affordable housing](https://opendata.dc.gov/datasets/DCGIS::affordable-housing/about) projects and [ward demographic](https://opendata.dc.gov/datasets/DCGIS::wards-from-2022/about) data.

Bonus: find, download, and use more recent ward population data. (Remember to include it in your PR.) My cursory search found data as late as 2022.

## New instructions for submitting a PR with multiple files

Because you'll be working with multiple files, PRs become _slightly_ more complicated, so we're graduating to a new 'mini-repository' pattern:
- Make a new folder in `exercises/exercise04` with your last name (just like the suffix for your notebook file)
- Upload your notebook file, also appropriately named, into that folder
- Upload any other files you make/use, including `.py` and `.csv` files, into that folder, so everything is together in the same place

Ultimately, this will look a bit like this:
```
── exercises
    ├── exercise04
        ├── harvey
            ├── exercise04_harvey.ipynb
            ├── affordable_housing_calcs.py
            ├── affordable_housing.csv
            └── wards_from_2022.csv
```

**NOTE:** Yes, I realize this is a bit redundant because everyone will have copies of the same CSV files. This would never be a good idea for production coding--we would have one `data` directory, and everyone would draw from the same data. However, there are two reasons for all these copies in this case:
1. It's good practice to build a repository with all the parts your code needs to run.
    - In later weeks, when you  _don't_ all have the same data, it won't seem as redundant.
3. Having everything in one folder will make it easy for me to run your code on my computer.

## Hints
- You may want to join the population data _after_ summarizing the affordable housing data (i.e., join populations to sums of units). However, I could also see an approach where you join at the beginning, then aggregate the population column with a method called `first`
"""

# Load the affordable housing project data from affordable_housing.csv
from google.colab import drive
drive.mount('/content/drive')

import os

os.getcwd()

wd_path = '/content/drive/MyDrive/Colab Notebooks'
os.chdir(wd_path)

os.path.isfile('affordable_housing.csv')

import pandas as pd
housing_projects = pd.read_csv('affordable_housing.csv')

# Loads the ward populations from wards_from_2022.csv
ward_populations = pd.read_csv('wards_from_2022.csv')

# check work
# ward_populations.head()
# housing_projects.head()

# join the population data to the affordable housing data
pop_hous = pd.read_csv('wards_from_2022.csv')

# check column names in tables
# for name in pop_hous['NAME'].sort_values().unique():
#     if name in housing_projects['MAR_WARD'].unique():
#         print(f'{name} in both')
#     else:
#         print(f'{name} not in housing projects')

housing_projects_with_pops = pd.merge(housing_projects, pop_hous, left_on='MAR_WARD', right_on='NAME')

# check work
# housing_projects_with_pops.head()

# in a function, calculate which wards are producing disproportionately large and small number of housing units given their populations

def affordable_units_by_ward(housing_projects_with_pops):
  # filter data for relevant units and status
  filtered_df = housing_projects_with_pops[housing_projects_with_pops['STATUS_PUBLIC'].isin(['Under Construction', 'Pipeline'])]
  affordable_units_df = filtered_df[['MAR_WARD', 'P0010001','AFFORDABLE_UNITS_AT_0_30_AMI', 'AFFORDABLE_UNITS_AT_31_50_AMI', 'AFFORDABLE_UNITS_AT_51_60_AMI']]
  affordable_units_df = affordable_units_df.rename(columns={'MAR_WARD':'Ward', 'P0010001' : 'Total Population'})


  # calculate total affordable units at 60% AMI for each ward
  affordable_units_df.loc[:, 'Affordable Units >= 60% AMI'] = affordable_units_df[['AFFORDABLE_UNITS_AT_0_30_AMI', 'AFFORDABLE_UNITS_AT_31_50_AMI', 'AFFORDABLE_UNITS_AT_51_60_AMI']].sum(axis=1)

  # group by ward and aggregate
  ward_totals = affordable_units_df.groupby('Ward').agg({
      'Affordable Units >= 60% AMI': 'sum',
      'Total Population': 'sum'
  })

  # calculate units per 1000 people (to be comprehendable)
  ward_totals['Units Per 1,000 People'] = ward_totals['Affordable Units >= 60% AMI'] / (ward_totals['Total Population']/ 1000)

  #sort in descending order
  ward_totals = ward_totals.sort_values(by='Units Per 1,000 People', ascending=False)

  # select desired columns
  return ward_totals[['Units Per 1,000 People']]

# call the function
result_df = affordable_units_by_ward(housing_projects_with_pops.copy())
result_df